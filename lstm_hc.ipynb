{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Reading data from: ./assays/processed/CHEMBL1909203.csv\n",
      "AUCs:\n",
      "Split1: 0.7703241895261845\n",
      "Split1_alt: 0.8049875311720699\n",
      "Split2: 0.7834164588528678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Split1': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                         criterion='gini', max_depth=None, max_features='auto',\n",
       "                         max_leaf_nodes=None, max_samples=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "                         oob_score=False, random_state=None, verbose=0,\n",
       "                         warm_start=False),\n",
       "  'Split1_alt': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                         criterion='gini', max_depth=None, max_features='auto',\n",
       "                         max_leaf_nodes=None, max_samples=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "                         oob_score=False, random_state=None, verbose=0,\n",
       "                         warm_start=False),\n",
       "  'Split2': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                         criterion='gini', max_depth=None, max_features='auto',\n",
       "                         max_leaf_nodes=None, max_samples=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=5,\n",
       "                         oob_score=False, random_state=None, verbose=0,\n",
       "                         warm_start=False)},\n",
       " {'Split1': 0.7703241895261845,\n",
       "  'Split1_alt': 0.8049875311720699,\n",
       "  'Split2': 0.7834164588528678},\n",
       " (0.047505938242280284, 0.047505938242280284))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from time import gmtime, strftime, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import TPScoringFunction, calc_auc, ecfp, score\n",
    "\n",
    "def timestamp():\n",
    "    return strftime(\"%Y-%m-%d_%H:%M:%S\", gmtime())\n",
    "\n",
    "def fit_clfs(chid, n_estimators, n_jobs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        chid: which assay to use:\n",
    "        external_file:\n",
    "    \"\"\"\n",
    "    # read data and calculate ecfp fingerprints\n",
    "    assay_file = f'./assays/processed/{chid}.csv'\n",
    "    print(f'Reading data from: {assay_file}')\n",
    "    df = pd.read_csv(assay_file)\n",
    "    X = np.array(ecfp(df.smiles))\n",
    "    y = np.array(df.label)\n",
    "\n",
    "    # split in equally sized sets. Stratify to get same label distributions\n",
    "    X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "\n",
    "    balance = (np.mean(y1), np.mean(y2))\n",
    "\n",
    "    # train classifiers and store them in dictionary\n",
    "    clfs = {}\n",
    "    clfs['Split1'] = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, n_jobs=n_jobs)\n",
    "    clfs['Split1'].fit(X1, y1)\n",
    "\n",
    "    clfs['Split1_alt'] = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, n_jobs=n_jobs)\n",
    "    clfs['Split1_alt'].fit(X1, y1)\n",
    "\n",
    "    clfs['Split2'] = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, n_jobs=n_jobs)\n",
    "    clfs['Split2'].fit(X2, y2)\n",
    "\n",
    "    # calculate AUCs for the clfs\n",
    "    aucs = {}\n",
    "    aucs['Split1'] = calc_auc(clfs['Split1'], X2, y2)\n",
    "    aucs['Split1_alt'] = calc_auc(clfs['Split1_alt'], X2, y2)\n",
    "    aucs['Split2'] = calc_auc(clfs['Split2'], X1, y1)\n",
    "    print(\"AUCs:\")\n",
    "    for k, v in aucs.items():\n",
    "        print(f'{k}: {v}')\n",
    "\n",
    "    return clfs, aucs, balance\n",
    "\n",
    "fit_clfs('CHEMBL1909203', 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize(chid,\n",
    "#              n_estimators,\n",
    "#              n_jobs,\n",
    "#              external_file,\n",
    "#              n_external,\n",
    "#              seed,\n",
    "#              optimizer_args):\n",
    "\n",
    "\n",
    "chid='CHEMBL3888429'\n",
    "n_estimators=100\n",
    "n_jobs=8\n",
    "external_file='./data/guacamol_v1_test.smiles.can'\n",
    "n_external=3000\n",
    "seed=101\n",
    "\n",
    "    \n",
    "np.random.seed(seed)\n",
    "# config = locals()\n",
    "# print(locals())\n",
    "#set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join('./test', 'lstm_hc', chid, timestamp())\n",
    "os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: ./assays/processed/CHEMBL3888429.csv\n",
      "AUCs:\n",
      "Split1: 0.8218614718614718\n",
      "Split1_alt: 0.8124458874458874\n",
      "Split2: 0.8033948940793046\n"
     ]
    }
   ],
   "source": [
    "# config_file = os.path.join(results_dir, 'config.json')\n",
    "# with open(config_file, 'w') as f:\n",
    "#     json.dump(config, f)\n",
    "\n",
    "\n",
    "\n",
    "clfs, aucs, balance = fit_clfs(chid, n_estimators, n_jobs)\n",
    "results = {}\n",
    "results['AUC'] = aucs\n",
    "results['balance'] = balance\n",
    "\n",
    "clf_file = os.path.join(results_dir, 'classifiers.p')\n",
    "with open(clf_file, 'wb') as f:\n",
    "    pickle.dump(clfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting initial population...\n",
      "0 | max: 0.383 | avg: 0.263 | min: 0.197 | std: 0.050 | sum: 26.287 | 4.46 sec/gen | 22.40 mol/sec | 0.89 rest \n",
      "1 | max: 0.481 | avg: 0.299 | min: 0.240 | std: 0.050 | sum: 29.879 | 1.51 sec/gen | 66.35 mol/sec | 0.97 rest \n",
      "2 | max: 0.511 | avg: 0.336 | min: 0.275 | std: 0.049 | sum: 33.565 | 1.56 sec/gen | 64.18 mol/sec | 1.05 rest \n"
     ]
    }
   ],
   "source": [
    "from guacamol_baselines.graph_ga.goal_directed_generation import GB_GA_Generator\n",
    "\n",
    "optimizer_args=dict(smi_file='./data/guacamol_v1_train.smiles.can',\n",
    "                    population_size=100,\n",
    "                    offspring_size=200,\n",
    "                    generations=3,\n",
    "                    mutation_rate=0.01,\n",
    "                    n_jobs=-1,\n",
    "                    random_start=True,\n",
    "                    patience=5,\n",
    "                    canonicalize=False)\n",
    "\n",
    "optimizer = GB_GA_Generator(**optimizer_args)\n",
    "smiles_history = optimizer.generate_optimized_molecules(scoring_function, 100, get_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selecting initial population...\n",
      "Top 4:\n",
      "\t0.495: Cc1c(NC(=O)c2ccc(S(=O)(=O)N3CCCCCC3)cc2)c(=O)n(-c2ccccc2)n1C\n",
      "\t0.475: N#Cc1ccc(C(=O)Nc2ccc(C=Cc3ccnc4ccccc34)cc2)cc1\n",
      "\t0.465: COP(=O)(OC)C(OC(=O)COc1ccc(C#N)cc1)c1cccs1\n",
      "\t0.423: CC1(O)CC(C(=O)N2CC=C(c3ccc(C(=O)NC(=N)N)cc3C(F)(F)F)CC2)C(c2ccc(Cl)c(F)c2)C1\n",
      "Top 4:\n",
      "\t0.495: Cc1c(NC(=O)c2ccc(S(=O)(=O)N3CCCCCC3)cc2)c(=O)n(-c2ccccc2)n1C\n",
      "\t0.475: N#Cc1ccc(C(=O)Nc2ccc(C=Cc3ccnc4ccccc34)cc2)cc1\n",
      "\t0.465: COP(=O)(OC)C(OC(=O)COc1ccc(C#N)cc1)c1cccs1\n",
      "\t0.460: CN(CC(CCN1CCC2(CC1)CS(=O)(=O)c1ccccc12)c1ccc(Cl)c(Cl)c1)S(=O)(=O)c1cccs1\n",
      "Top 4:\n",
      "\t0.495: Cc1c(NC(=O)c2ccc(S(=O)(=O)N3CCCCCC3)cc2)c(=O)n(-c2ccccc2)n1C\n",
      "\t0.485: O=c1c(-c2cccc(F)c2)nc2cncnc2n1Cc1ccccc1F\n",
      "\t0.475: N#Cc1ccc(C(=O)Nc2ccc(C=Cc3ccnc4ccccc34)cc2)cc1\n",
      "\t0.465: COP(=O)(OC)C(OC(=O)COc1ccc(C#N)cc1)c1cccs1\n",
      "Top 4:\n",
      "\t0.537: N#Cc1ccc(-c2csc(NN=Cc3cccnc3)n2)cc1\n",
      "\t0.521: CN1CCN(S(=O)(=O)c2ccc(C(=O)Nc3ccc(Cl)c(S(=O)(=O)Nc4ccc(C#N)cc4)c3)cc2)CC1\n",
      "\t0.500: N#Cc1ccc(N2CCN(c3nc(Nc4cncnc4)nc(N4CCN(c5ccc(C(=O)O)cc5)CC4)n3)CC2)cc1\n",
      "\t0.495: Cc1c(NC(=O)c2ccc(S(=O)(=O)N3CCCCCC3)cc2)c(=O)n(-c2ccccc2)n1C\n",
      "Top 4:\n",
      "\t0.560: O=C(COc1ccc(S(=O)(=O)N2CCN(c3ccccn3)CC2)cc1)c1ccccc1\n",
      "\t0.540: COc1ccc(C=CC(=O)Nc2cc(S(=O)(=O)N3CCN(c4ccccc4)CC3)ccc2OS(=O)(=O)c2cccs2)cc1\n",
      "\t0.537: N#Cc1ccc(-c2csc(NN=Cc3cccnc3)n2)cc1\n",
      "\t0.533: COC1C(C(O)CNC(C)(C)C)OC(O)(c2ccc3cc(Cl)ccc3c2)C1Oc1c(Cl)cc(C#N)cc1Br\n",
      "Top 4:\n",
      "\t0.580: O=C(C1CC1c1cccs1)N1CCC2(CCN(Cc3cccnc3)CC2)CC1\n",
      "\t0.580: CN(C)C(=O)c1cc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)c2ccccn2c1=O\n",
      "\t0.575: CN(Cc1cccs1)C(=O)c1ccc(OC2CCN(Cc3cccnc3)CC2)s1\n",
      "\t0.560: O=C(COc1ccc(S(=O)(=O)N2CCN(c3ccccn3)CC2)cc1)c1ccccc1\n",
      "Top 4:\n",
      "\t0.580: O=C(C1CC1c1cccs1)N1CCC2(CCN(Cc3cccnc3)CC2)CC1\n",
      "\t0.580: CN(C)C(=O)c1cc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)c2ccccn2c1=O\n",
      "\t0.575: CN(Cc1cccs1)C(=O)c1ccc(OC2CCN(Cc3cccnc3)CC2)s1\n",
      "\t0.560: O=C(COc1ccc(S(=O)(=O)N2CCN(c3ccccn3)CC2)cc1)c1ccccc1\n",
      "Top 4:\n",
      "\t0.609: O=C(CCNC(=O)c1ccc(S(=O)(=O)N2CCN(c3ccc(F)cc3)CC2)cc1)NN=C(NN=Cc1cccnc1)C(F)(F)F\n",
      "\t0.605: N#Cc1ccc(NC(=O)CCC2CCN(C(=O)CN3CC(=O)C(N(CCO)Cc4cccs4)C3=O)CC2)cc1\n",
      "\t0.600: N#Cc1csc(NC(=S)Nc2cc(S(=O)(=O)N3CCN(c4cccnc4)CC3)ccc2S(=O)(=O)c2cccs2)c1\n",
      "\t0.600: N#CCC1(n2cc(C(=N)O)c(=Nc3ccnc(F)c3)[nH]2)CCN(S(=O)(=O)c2ccccc2)CC1\n",
      "Top 4:\n",
      "\t0.676: CN(C(=O)c1ccc(S(=O)(=O)Nc2ccc(C#N)cc2)cc1)C1CCN(CC2(O)CCN(C(=O)OC(C)(C)C)CC2)CC1\n",
      "\t0.673: N#Cc1ccc(C(=O)Nc2ccc(C(=O)N3CCN(S(=O)(=O)c4ccc(C(F)(F)F)nc4)CC3)cc2)cc1\n",
      "\t0.640: N#Cc1ccc(C(=S)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)cc1\n",
      "\t0.618: N#Cc1ccc(OC2C=CN(S(=O)(=O)C3=CC=C(CN(CC(F)(F)F)c4cccnc4)CC3)CC2)cc1\n",
      "Top 4:\n",
      "\t0.676: CN(C(=O)c1ccc(S(=O)(=O)Nc2ccc(C#N)cc2)cc1)C1CCN(CC2(O)CCN(C(=O)OC(C)(C)C)CC2)CC1\n",
      "\t0.673: N#Cc1ccc(C(=O)NC(=S)Nc2ccc(S(=O)(=O)N3CCN(c4ccc(C(F)(F)F)cc4)CC3)cc2)cc1\n",
      "\t0.673: N#Cc1ccc(C(=O)Nc2ccc(C(=O)N3CCN(S(=O)(=O)c4ccc(C(F)(F)F)nc4)CC3)cc2)cc1\n",
      "\t0.672: N#Cc1ccc(C2(C(=O)c3cccs3)CCN(S(=O)(=O)c3ccc(C(F)(F)F)cc3)CC2)cc1\n",
      "Top 4:\n",
      "\t0.688: N#Cc1ccc(C(=O)NCC(=O)N2CCN(S(=O)(=O)c3ccc(NC(=O)c4cccs4)cc3)CC2)cc1\n",
      "\t0.684: N#Cc1ccc(NC(=S)N2CCN(S(=O)(=O)c3ccc(O)c(OC4OC(C(F)(F)F)C(O)C(O)C4O)c3)CC2)cc1\n",
      "\t0.680: N#Cc1ccc(C(C(=O)NC(CC(=O)Nc2ccc(C#N)cn2)C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)c2cccnc2)cc1\n",
      "\t0.676: CN(C(=O)c1ccc(S(=O)(=O)Nc2ccc(C#N)cc2)cc1)C1CCN(CC2(O)CCN(C(=O)OC(C)(C)C)CC2)CC1\n",
      "Top 4:\n",
      "\t0.742: N#Cc1ccc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccc(S(=O)(=O)N5CCN(c6ccc(C#N)cc6)CC5)cc4)C=C3C(F)(F)F)cc2)cc1\n",
      "\t0.688: N#Cc1ccc(C(=O)NCC(=O)N2CCN(S(=O)(=O)c3ccc(NC(=O)c4cccs4)cc3)CC2)cc1\n",
      "\t0.684: N#Cc1ccc(NC(=S)N2CCN(S(=O)(=O)c3ccc(O)c(OC4OC(C(F)(F)F)C(O)C(O)C4O)c3)CC2)cc1\n",
      "\t0.683: N#Cc1ccc(C(=O)Nc2ccc(C(=O)N3CCN(S(=O)(=O)c4ccc(OC(F)(F)F)cc4)CC3)cc2)cc1\n",
      "Top 4:\n",
      "\t0.742: N#Cc1ccc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccc(S(=O)(=O)N5CCN(c6ccc(C#N)cc6)CC5)cc4)C=C3C(F)(F)F)cc2)cc1\n",
      "\t0.703: N#Cc1ccc(Nc2nc(C(F)(F)F)ccc2C(=O)Nc2ccc(C(=O)N3CCN(S(=O)(=O)c4cccs4)CC3)cc2)cc1\n",
      "\t0.698: N#Cc1ccc(C(=O)N(CCC(O)c2ccc(F)c(F)c2)C(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)c2cccnc2)cc1\n",
      "\t0.690: N#Cc1ccc(Nc2nc(N3CCN(C(=O)c4cccs4)CC3)cc(N3CCN(c4ccccn4)CC3)n2)cc1\n",
      "Top 4:\n",
      "\t0.742: N#Cc1ccc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccc(S(=O)(=O)N5CCN(c6ccc(C#N)cc6)CC5)cc4)C=C3C(F)(F)F)cc2)cc1\n",
      "\t0.715: N#Cc1ccc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(S(=O)(=O)c4ccc(OC(F)(F)F)c(O)c4)CC3)cc2)cc1\n",
      "\t0.710: N#Cc1ccc(C(c2cccnc2)N(CC(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)S(=O)(=O)c2cccs2)cc1\n",
      "\t0.703: N#Cc1ccc(Nc2nc(C(F)(F)F)ccc2C(=O)Nc2ccc(C(=O)N3CCN(S(=O)(=O)c4cccs4)CC3)cc2)cc1\n",
      "Top 4:\n",
      "\t0.742: N#Cc1ccc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccc(S(=O)(=O)N5CCN(c6ccc(C#N)cc6)CC5)cc4)C=C3C(F)(F)F)cc2)cc1\n",
      "\t0.715: N#Cc1ccc(C(=O)Nc2ccc(S(=O)(=O)N3CCN(S(=O)(=O)c4ccc(OC(F)(F)F)c(O)c4)CC3)cc2)cc1\n",
      "\t0.710: N#Cc1ccc(C(c2cccnc2)N(CC(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)S(=O)(=O)c2cccs2)cc1\n",
      "\t0.710: N#Cc1ccc(C(c2cccnc2)N(CC(=O)Nc2ccc(S(=O)(=O)N3CCN(c4ccccn4)CC3)cc2)S(=O)(=O)c2cccs2)c(F)c1\n"
     ]
    }
   ],
   "source": [
    "from guacamol_baselines.smiles_lstm_hc.smiles_rnn_directed_generator import SmilesRnnDirectedGenerator\n",
    "\n",
    "# Create guacamol scoring function with clf trained on split 1\n",
    "scoring_function = TPScoringFunction(clfs['Split1'])\n",
    "\n",
    "optimizer_args = dict(pretrained_model_path = './guacamol_baselines/smiles_lstm_hc/pretrained_model/model_final_0.473.pt',\n",
    "                        n_epochs = 150,\n",
    "                        mols_to_sample = 1028,\n",
    "                        keep_top = 512,\n",
    "                        optimize_n_epochs = 1,\n",
    "                        max_len = 100,\n",
    "                        optimize_batch_size = 64,\n",
    "                        number_final_samples = 1028,\n",
    "                        sample_final_model_only = False,\n",
    "                        random_start = True,\n",
    "                        smi_file = './data/guacamol_v1_train.smiles.can',\n",
    "                        n_jobs = -1,\n",
    "                        canonicalize=False)\n",
    "\n",
    "# run optimization\n",
    "t0 = time()\n",
    "optimizer = SmilesRnnDirectedGenerator(**optimizer_args)\n",
    "final_mols, smiles_history = optimizer.generate_optimized_molecules(scoring_function, 100, get_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_list(smiles):\n",
    "    ms = [Chem.MolFromSmiles(s) for s in smiles]\n",
    "    return [Chem.MolToSmiles(m) for m in ms if m is not None]\n",
    "\n",
    "smiles_history = [can_list(e) for e in smiles_history]\n",
    "\n",
    "min_length = min(len(e) for e in smiles_history)\n",
    "smiles_history = [e[:min_length] for e in smiles_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 1/15 [00:03<00:46,  3.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 2/15 [00:06<00:43,  3.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 3/15 [00:10<00:40,  3.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 4/15 [00:13<00:37,  3.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 5/15 [00:16<00:33,  3.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 6/15 [00:20<00:30,  3.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 7/15 [00:23<00:27,  3.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 8/15 [00:27<00:23,  3.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 9/15 [00:30<00:20,  3.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 10/15 [00:34<00:17,  3.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 11/15 [00:37<00:13,  3.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 12/15 [00:41<00:10,  3.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 13/15 [00:44<00:06,  3.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 14/15 [00:47<00:03,  3.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n",
      "787\n",
      "787\n",
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [00:51<00:00,  3.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make a list of dictionaries for every time step\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "statistics = []\n",
    "for optimized_smiles in tqdm(smiles_history):\n",
    "    row = {}\n",
    "    row['smiles'] = optimized_smiles\n",
    "    print(sum(1 for s in optimized_smiles if Chem.MolFromSmiles(s)))\n",
    "    row['preds'] = {}\n",
    "    for k, clf in clfs.items():\n",
    "        preds = score(optimized_smiles, clf)\n",
    "        print(sum(1 for s in preds if s is not None))\n",
    "        row['preds'][k] = preds\n",
    "    statistics.append(row)\n",
    "\n",
    "results['statistics'] = statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing results in ./test/lstm_hc/CHEMBL3888429/2020-03-05_15:43:18\n"
     ]
    }
   ],
   "source": [
    "with open(external_file) as f:\n",
    "    external_smiles = f.read().split()[]\n",
    "external_smiles = np.random.choice(external_smiles, n_external)\n",
    "results['predictions_external'] = {k: score(external_smiles, clf) for k, clf in clfs.items()}\n",
    "\n",
    "results_file = os.path.join(results_dir, 'results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(f'Storing results in {results_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Goal-directed generation benchmark for SMILES RNN',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--model_path', default=None, help='Full path to the pre-trained SMILES RNN model')\n",
    "parser.add_argument('--max_len', default=100, type=int, help='Max length of a SMILES string')\n",
    "parser.add_argument('--seed', default=42, type=int, help='Random seed')\n",
    "parser.add_argument('--output_dir', default=None, help='Output directory for results')\n",
    "parser.add_argument('--number_repetitions', default=1, type=int, help='Number of re-training runs to average')\n",
    "parser.add_argument('--keep_top', default=512, type=int, help='Molecules kept each step')\n",
    "parser.add_argument('--n_epochs', default=20, type=int, help='Epochs to sample')\n",
    "parser.add_argument('--mols_to_sample', default=1024, type=int, help='Molecules sampled at each step')\n",
    "parser.add_argument('--optimize_batch_size', default=256, type=int, help='Batch size for the optimization')\n",
    "parser.add_argument('--optimize_n_epochs', default=2, type=int, help='Number of epochs for the optimization')\n",
    "parser.add_argument('--benchmark_num_samples', default=4096, type=int,\n",
    "                    help='Number of molecules to generate from final model for the benchmark')\n",
    "parser.add_argument('--benchmark_trajectory', action='store_true',\n",
    "                    help='Take molecules generated during re-training into account for the benchmark')\n",
    "parser.add_argument('--smiles_file', default='data/guacamol_v1_all.smiles')\n",
    "parser.add_argument('--random_start', action='store_true')\n",
    "parser.add_argument('--n_jobs', type=int, default=-1)\n",
    "parser.add_argument('--suite', default='v2')\n",
    "\n",
    "args = parser.parse_args('')\n",
    "for k,v in args.__dict__.items():\n",
    "    print(f'{k} = {v},')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guacamol",
   "language": "python",
   "name": "guacamol"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
